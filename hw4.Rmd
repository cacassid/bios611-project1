---
title: "Homework 4"
author: "Caitlin Cassidy"
date: "10/8/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1:

Build a glm in R to classify individuals as either Male or Female based on their weight and height. What is the accuracy of the model?

```{r q1}
#read in csv
index <- read.csv("hw_data/500_Person_Gender_Height_Weight_Index.csv", header = TRUE)

#function to split data into train test validate
model_split <- function(dfi, train_p, validate_p, test_p, col_name="exp_group"){
    dfi <- sample_n(dfi, nrow(dfi),replace=FALSE);
    p <- (seq(nrow(dfi))-1)/nrow(dfi);
    train_dfi <- dfi %>% filter(p < train_p);
    validate_dfi <- dfi %>% filter(p < train_p + validate_p & p >= train_p);
    test_dfi <- dfi %>% filter(p >= train_p + validate_p);
    train_dfi[[col_name]] <- "train";
    validate_dfi[[col_name]] <- "validate";
    test_dfi[[col_name]] <- "test";
    rbind(train_dfi, validate_dfi, test_dfi);
}

#function to get f1 score
f1 <- function (y_true, y_pred, positive = NULL) 
{
    Confusion_DF <- ConfusionDF(y_pred, y_true)
    if (is.null(positive) == TRUE) 
        positive <- as.character(Confusion_DF[1, 1])
    Precision <- Precision(y_true, y_pred, positive)
    Recall <- Recall(y_true, y_pred, positive)
    F1_Score <- 2 * (Precision * Recall)/(Precision + Recall)
    return(F1_Score)
}

#f1 <- MLmetrics::F1_Score

#create indicator variable for gender
index <- index %>% mutate(gender1 = ifelse(Gender == "Male", 0, 1))

#split data into train test validate
index <- rbind(model_split(index, 1/3, 1/3, 1/3))
index %>% group_by(exp_group) %>% tally()

#split data into 3 groups
train_1 <- index %>% filter(exp_group=="train")
validate_1 <- index %>% filter(exp_group=="validate")
test_1 <- index %>% filter(exp_group=="test")

#create logistic model on train data
model <- glm(gender1 ~ Height + Weight, family=binomial(link='logit'), data=train_1)

#
pred <- predict(model, newdata=validate_1, type="response")
pred
sum((pred>0.5) == validate_1$gender1)/nrow(validate_1)

#f1 score
#f1(validate_1$gender1, pred > 0.5)
```

This model has an accuracy of about 44.9%.
***We get a different accuracy each time because the groups change***

## Problem 2:

Use the ‘gbm’ package to train a similar model. Don’t worry about hyper parameter tuning for now.
What is the accuracy of the model?

```{r q2}
#install.packages("gbm")
library(gbm)

model_2 <- gbm(gender1 ~ Height + Weight, distribution="bernoulli",
             data=train_1,
             n.trees = 100,
             interaction.depth = 2,
             shrinkage = 0.1)

pred <- predict(model_2, validate_1, type="response")
sum((pred>0.5)==validate_1$gender1)/nrow(validate_1)

```
The accuracy of this model is about 48.5%

## Problem 3:

Filter the data set so that it contains only 50 Male examples. Create a new model for this data set. What is the F1 Score of the model?

```{r q3}
#filter data set to get only 50 male examples
index50men <- index %>% filter(Gender == "Male")
index50men <- index50men[1:50,]
indexwomen <- index %>% filter(Gender == "Female")
index50 <- rbind(index50men, indexwomen)

index50_2 <- index50[sample(nrow(index50)),]

#function to split data into train test validate
model_split <- function(dfi, train_p, validate_p, test_p, col_name="exp_group"){
    dfi <- sample_n(dfi, nrow(dfi),replace=FALSE);
    p <- (seq(nrow(dfi))-1)/nrow(dfi);
    train_dfi <- dfi %>% filter(p < train_p);
    validate_dfi <- dfi %>% filter(p < train_p + validate_p & p >= train_p);
    test_dfi <- dfi %>% filter(p >= train_p + validate_p);
    train_dfi[[col_name]] <- "train";
    validate_dfi[[col_name]] <- "validate";
    test_dfi[[col_name]] <- "test";
    rbind(train_dfi, validate_dfi, test_dfi);
}


#split data into train test validate
index50_2 <- rbind(model_split(index50_2, 1/3, 1/3, 1/3))
index50_2 %>% group_by(exp_group) %>% tally()

#create new model for this data
train_3 <- index50_2 %>% filter(exp_group=="train")
validate_3 <- index50_2 %>% filter(exp_group=="validate")
test_3 <- index50_2 %>% filter(exp_group=="test")
model_3 <- gbm(gender1 ~ Height + Weight, distribution="bernoulli",
             data=train_3,
             n.trees = 100,
             interaction.depth = 2,
             shrinkage = 0.1)

pred_3 <- predict(model_3, validate_3, type="response")
pred_3

sum((pred>0.5)==validate_3$gender1)/nrow(validate_3)

#find f1 score
#install.packages("MLmetrics")
library(MLmetrics)

#F1_Score(validate_3$gender1, pred > 0.5)

tp = mean(pred_3[validate_3$gender1])
fp = mean(pred_3[!validate_3$gender1])
fn = fp = mean(1 - pred_3[!validate_3$gender1])

f1_calc = tp/(tp+0.5*(fp+fn))
f1_calc

```

The F1 score is 

## Problem 4:

For the model in the previous example plot an ROC curve. What does this ROC curve mean?

```{r q4}
#make tibble to plot ROC curve
roc <- do.call(rbind, Map(function(threshold){
    p <- pred_3 > threshold;
    tp <- sum(p[validate_3$gender1])/sum(validate_3$gender1);
    fp <- sum(p[!validate_3$gender1])/sum(!validate_3$gender1);
    tibble(threshold=threshold,
           tp=tp,
           fp=fp)
},seq(100)/100))

#plot ROC curve
ggplot(roc, aes(fp,tp)) + geom_line() + xlim(0,1) + ylim(0,1) +
    labs(title="ROC Curve",x="False Positive Rate",y="True Positive Rate")
```
True positive rate = how often we predict a male as a male
False positive rate = how often we predict a female as a male

## Problem 5:
Using K-Means, cluster the same data set. Can you identify the clusters with the known labels? Provide an interpretation of this result.

```{r q5}
library(Rtsne)
#source("utils.R")
index_k_means <- index50 %>% select(-Gender, -exp_group) %>% distinct()
## use whole data set not 50 male data set 

#choose number of clusters
library(cluster)
results <- clusGap(index %>%
                 select(Height, Weight, Index, gender1),
                 kmeans,
                 K.max = 10,
                 B = 500)
ggplot(results$Tab %>% as_tibble() %>%
       mutate(k=seq(nrow(.))), aes(k,gap)) + geom_line()

#k means
cc <- kmeans(index_k_means, 3)
cc
fit <- Rtsne(index_k_means, dims = 3)
fit
ggplot(fit$Y %>% as.data.frame() %>% as_tibble() %>% mutate(label=cc$cluster),aes(V1,V2)) +
    geom_point(aes(color=factor(label)))

```
From the gap plot, k = 3 looks to be the best number of clusters.

How to identify the clusters?
