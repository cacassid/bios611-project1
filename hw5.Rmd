---
title: "hw5"
author: "Caitlin Cassidy"
date: "10/27/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1

Repeat your GBM model. Contrast your results with the results for the previous exercise.

```{r p1}
library(gbm)
#read in csv
hw<- read.csv("hw_data/datasets_26073_33239_weight-height.csv", header = TRUE)

#function to split data into train test validate
model_split <- function(dfi, train_p, validate_p, test_p, col_name="exp_group"){
    dfi <- sample_n(dfi, nrow(dfi),replace=FALSE);
    p <- (seq(nrow(dfi))-1)/nrow(dfi);
    train_dfi <- dfi %>% filter(p < train_p);
    validate_dfi <- dfi %>% filter(p < train_p + validate_p & p >= train_p);
    test_dfi <- dfi %>% filter(p >= train_p + validate_p);
    train_dfi[[col_name]] <- "train";
    validate_dfi[[col_name]] <- "validate";
    test_dfi[[col_name]] <- "test";
    rbind(train_dfi, validate_dfi, test_dfi);
}

#create indicator variable for gender
hw <- hw %>% mutate(gender1 = ifelse(Gender == "Male", 0, 1))

#split data into train test validate
hw <- rbind(model_split(hw, 1/3, 1/3, 1/3))
hw %>% group_by(exp_group) %>% tally()

#split data into 3 groups
train_1 <- hw %>% filter(exp_group=="train")
validate_1 <- hw %>% filter(exp_group=="validate")
test_1 <- hw %>% filter(exp_group=="test")


model_gbm <- gbm(gender1 ~ Height + Weight, distribution="bernoulli",
             data=train_1,
             n.trees = 100,
             interaction.depth = 2,
             shrinkage = 0.1)

pred <- predict(model_2, validate_1, type="response")
sum((pred>0.5)==validate_1$gender1)/nrow(validate_1)
```
The accuracy of the model using this new data set is slightly better at 48% vs 44%.

## Problem 2

1. Examine the dataset for any irregularities. Make the case for filtering out a subset of rows (or for not doing so).
```{r 2.1}
#read in csv
heroes <- read.csv("hw_data/datasets_38396_60978_charcters_stats.csv", header = TRUE)
heroes <- heroes %>% filter(Total != 5)
```
There are many characters with a total of 5, which seems like an error. I will filter out these rows because so many have the exact same values for all powers.

2. Perform a principal component analysis on the numerical columns of this data. How many components do we need to get 85% of the variation in the data set?
```{r 2.2}
#just numerical columns
heroes2 <- heroes %>% select(-Name, - Alignment)

#PCA
pcs <- prcomp(heroes2)
summary(pcs)
```
We just need 1 component to get 85% of the variation. (This does include the total variable)

3. Do we need to normalize these columns or not?

We should normalize the columns because the variance should be on the same scale.

4. Is the "total" column really the total of the values in the other columns?
```{r 2.4}
heroes <- heroes %>% mutate(total_check = Intelligence+Strength+Speed+Durability+Power+Combat)
heroes <- heroes %>% mutate(total_diff = Total - total_check)
sum(heroes$total_diff)
```
Yes. 

5. Should we have included in in the PCA? What do you expect about the largest principal components and the total column? Remember, a given principal component corresponds to a weighted combination of the original variables.

No, we should not include the total column because it is basically repeating information from the other columns.

6. Make a plot of the two largest components. Any insights?
```{r 2.6}
d <- dist(heroes2);
fit <- cmdscale(d, eig=TRUE, k=2);
ggplot(fit$points %>%
       as.data.frame() %>%
       as_tibble() %>%
       mutate(label=data$label), aes(V1,V2)) + geom_point(aes(color=label));
```

## Problem 3

Use Python/sklearn to perform a TSNE dimensionality reduction (to two dimensions) on the numerical columns from the set above. 
Once youâ€™ve performed the analysis in Python (feel free to use a Python notebook) write the results to a csv file and load them into R. In R, plot the results.
Color each point by the alignment of the associated character. Any insights?

```{r q3}
lowd <- read.csv("hw_data/lowd.csv", header = TRUE)


ggplot(lowd, aes(X1,X2)) + geom_point(aes(color=cluster))
```
There seem to be two main clusters that correspond to the good and bad characters.

## Problem 4

Reproduce your plot in Python with plotnine (or the library of your choice)

(See python file hw5.ipynb)

## Problem 5

Using the Caret library, train a GBM model which attempts to predict character alignment. What are the final parameters that caret determines are best for the model?

Hints: you want to use the "train" method with the "gbm" method. Use "repeatedcv" for the characterization method. If this is confusing, don't forget to read the Caret docs.
```{r q5}
#install.packages("e1071")
#install.packages("caret")
library(caret)
library(e1071)

heroes3 <- heroes %>% select(-Name, - Total)

trainIndex <- createDataPartition(heroes3$Alignment, p = .8, 
                                  list = FALSE, 
                                  times = 1)

heroes3$Alignment <- factor(heroes3$Alignment)

form <- Alignment ~ Intelligence + Strength + Speed + Durability + Power + Combat

train_ctrl <- trainControl(method = "repeatedcv", number = 50)
gbmFit1 <- train(form, data = heroes3 %>% slice(trainIndex), 
                 method = "gbm", 
                 trControl = train_ctrl,
                 verbose = FALSE)

summary(gbmFit1)

```
The final parameters that are best for the model are combat, durability and strength.

## Problem 6

A conceptual question: why do we need to characterize our models using strategies like k-fold cross validation? Why can't we just report a single number for the accuracy of our model?

The point of using k-fold cross validation is to examine the model's performance on folds of data that were not used in training the model. This is important because it will tell us how good the model actually can predict new data rather than how well the model can be fit to data we use to train it. Reporting the accuracy as a single number isn't enough information to decide if the model is any good or not because there are many more factors to consider. 

## Problem 7

Describe in words the process of recursive feature elimination

Recursive feature elimination is a method for feature selection. It works by searching for a subset of features in the training data set and removing features until a good number of features is left. It uses recursion to to consider smaller and smaller numbers of features to determine which features are most important.